{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb95187-f7c3-4aad-a0ea-eb87945ac27c",
   "metadata": {},
   "source": [
    "# Theoretical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f218f-83f5-4370-9486-cd2cee69276b",
   "metadata": {},
   "source": [
    "##\n",
    "### 1. What is a Support Vector Machine (SVM) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feeb417-687c-40e1-90c7-dfa6b549f724",
   "metadata": {},
   "source": [
    "A **Support Vector Machine (SVM)** is a **supervised machine learning algorithm** used for **classification and regression tasks**. It works by finding the **optimal hyperplane** that best separates data points of different classes in an N-dimensional space.\n",
    "\n",
    "* The **support vectors** are the data points closest to the decision boundary — they define the position of the hyperplane.\n",
    "* SVM aims to **maximize the margin**, i.e., the distance between the hyperplane and the nearest data points from each class.\n",
    "* It can also handle **non-linear data** using **kernel functions** (e.g., polynomial, RBF).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fd88f9-3ab7-4277-b311-7b3bdae7252a",
   "metadata": {},
   "source": [
    "##\n",
    "### 2. What is the difference between Hard Margin and Soft Margin SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284343b-30c8-4c91-bf8e-7ca904d6ff4a",
   "metadata": {},
   "source": [
    "**Hard Margin SVM:**\n",
    "\n",
    "* Assumes that data is **perfectly linearly separable**.\n",
    "* Finds a hyperplane that separates all points **without any misclassification**.\n",
    "* Very **sensitive to noise and outliers**, as even one incorrect point can make separation impossible.\n",
    "\n",
    "**Soft Margin SVM:**\n",
    "\n",
    "* Allows some **misclassifications** by introducing a **slack variable (ξ)**.\n",
    "* Balances between maximizing the margin and minimizing classification errors.\n",
    "* More **robust** and works well with **real-world, noisy data**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0eb87e-456e-4099-8b8b-e8857384f8a5",
   "metadata": {},
   "source": [
    "##\n",
    "### 3. What is the mathematical intuition behind SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08005e8c-2b80-4675-9d8c-e8ec7718fad4",
   "metadata": {},
   "source": [
    "The **mathematical intuition** behind SVM is to find a **hyperplane** that best separates two classes by **maximizing the margin** — the distance between the hyperplane and the nearest data points (support vectors).\n",
    "\n",
    "For a dataset with features ( X ) and labels ( y ):\n",
    "\n",
    "$$[\n",
    "y_i (w \\cdot x_i + b) \\ge 1\n",
    "]$$\n",
    "\n",
    "SVM minimizes the cost function:\n",
    "\n",
    "$$[\n",
    "\\min_{w,b} \\ \\frac{1}{2} |w|^2\n",
    "]$$\n",
    "\n",
    "subject to the above constraint.\n",
    "\n",
    "* ( w ) → weight vector defining the hyperplane\n",
    "* ( b ) → bias term\n",
    "* Maximizing margin ( = \\frac{2}{|w|} )\n",
    "\n",
    "\n",
    "👉 In soft margin SVM, a penalty term ( C \\sum \\xi_i ) is added to allow some misclassifications while maintaining the widest possible margin.\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70432ca-45c0-48e4-bc44-287ef5678fa0",
   "metadata": {},
   "source": [
    "##\n",
    "### 4. What is the role of Lagrange Multipliers in SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a9da2d-4b4f-4eed-a6d7-c890ac70e401",
   "metadata": {},
   "source": [
    "**Lagrange Multipliers** are used in SVM to transform the **constrained optimization problem** (finding the optimal hyperplane with maximum margin) into an **unconstrained one**, making it easier to solve mathematically.\n",
    "\n",
    "They help incorporate the constraints:\n",
    "\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\ge 1\n",
    "$$\n",
    "\n",
    "into the objective function through a **Lagrangian formulation**:\n",
    "\n",
    "$$\n",
    "L(w, b, \\alpha) = \\frac{1}{2}|w|^2 - \\sum_i \\alpha_i [y_i(w \\cdot x_i + b) - 1]\n",
    "$$\n",
    "\n",
    "where ( \\alpha_i ) are the Lagrange multipliers.\n",
    "\n",
    "* Non-zero ( \\alpha_i ) correspond to **support vectors**.\n",
    "* This formulation leads to the **dual problem**, which allows SVM to efficiently handle **non-linear data** using **kernel tricks**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628db753-8bae-40be-9f8a-fc9dd6b7b196",
   "metadata": {},
   "source": [
    "##\n",
    "### 5.  What are Support Vectors in SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe62c492-5ff6-4a27-a6dd-222287013c0b",
   "metadata": {},
   "source": [
    "**Support Vectors** are the **data points that lie closest to the decision boundary (hyperplane)** in an SVM model.\n",
    "\n",
    "* They are the **critical elements** of the training set that directly influence the position and orientation of the hyperplane.\n",
    "* Only these points are used to define the margin — removing them would **change the decision boundary**, whereas removing others would not.\n",
    "* Support vectors are identified by **non-zero Lagrange multipliers (αᵢ)** in the optimization process.\n",
    "\n",
    "👉 They ensure the SVM achieves **maximum margin separation** between classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf2b53-afdb-4651-a52f-4b085cf43081",
   "metadata": {},
   "source": [
    "##\n",
    "### 6. What is a Support Vector Classifier (SVC) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be992e2-509f-494f-8964-7a29dcf2bf23",
   "metadata": {},
   "source": [
    "A **Support Vector Classifier (SVC)** is the **classification implementation of SVM** used to separate data into distinct categories. It finds the **optimal hyperplane** that maximizes the margin between different classes.\n",
    "\n",
    "* It supports both **linear and non-linear classification** using **kernel functions** (e.g., linear, polynomial, RBF).\n",
    "* The **regularization parameter (C)** controls the trade-off between maximizing margin and minimizing misclassification.\n",
    "* SVC is robust to high-dimensional data and effective when the **number of features exceeds the number of samples**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2587d22f-eb39-4496-9a3c-2f3b95c21163",
   "metadata": {},
   "source": [
    "##\n",
    "### 7. What is a Support Vector Regressor (SVR) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7ae36-9308-4796-b996-54f4fa6f2952",
   "metadata": {},
   "source": [
    "A **Support Vector Regressor (SVR)** is the **regression version of SVM**, designed to predict continuous values rather than class labels.\n",
    "\n",
    "* It tries to fit a function within a **margin of tolerance (ε)** around the actual data points.\n",
    "* Errors within this margin are **ignored**, while points outside are **penalized** using a regularization parameter ( C ).\n",
    "* SVR aims to find a **flat and simple function** that approximates data well, making it robust to **outliers** and effective for **non-linear regression** using kernel functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5413c8f8-b48c-4241-b11d-e967438eae9d",
   "metadata": {},
   "source": [
    "##\n",
    "### 8. What is the Kernel Trick in SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1f9264-fb13-4c29-9ae8-ac755f45da10",
   "metadata": {},
   "source": [
    "The **Kernel Trick** allows SVM to handle **non-linearly separable data** by implicitly mapping input features into a **higher-dimensional space** where a linear separator can be found — **without explicitly computing the transformation**.\n",
    "\n",
    "Common kernel functions:\n",
    "\n",
    "* **Linear Kernel:** ( K(x, x') = x \\cdot x' )\n",
    "* **Polynomial Kernel:** ( K(x, x') = (x \\cdot x' + 1)^d )\n",
    "* **RBF (Gaussian) Kernel:** ( K(x, x') = e^{-\\gamma |x - x'|^2} )\n",
    "\n",
    "👉 This trick enables SVM to model **complex, non-linear relationships** efficiently while keeping computation feasible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca7cf4-2e06-4b30-92bf-d57870417957",
   "metadata": {},
   "source": [
    "##\n",
    "### 9.  Compare Linear Kernel, Polynomial Kernel, and RBF Kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883f24f-fe94-49d1-b5bd-cb0fd1dba7d8",
   "metadata": {},
   "source": [
    "| **Kernel Type**           | **Mathematical Form**                 | **Use Case**                               | **Advantages**                              | **Limitations**                                 |\n",
    "| ------------------------- | ------------------------------------- | ------------------------------------------ | ------------------------------------------- | ----------------------------------------------- |\n",
    "| **Linear Kernel**         | ( K(x, x') = x \\cdot x' )             | When data is **linearly separable**        | Simple, fast, easy to interpret             | Fails with non-linear data                      |\n",
    "| **Polynomial Kernel**     | ( K(x, x') = (x \\cdot x' + 1)^d )     | When data has **moderate non-linearity**   | Captures interactions and curved boundaries | Sensitive to degree (d), can overfit            |\n",
    "| **RBF (Gaussian) Kernel** | ( K(x, x') = e^{-\\gamma |x - x'|^2} ) | For **highly non-linear** and complex data | Powerful and flexible for most cases        | Requires careful tuning of ( \\gamma ) and ( C ) |\n",
    "\n",
    "👉 In practice, **RBF Kernel** is the most widely used due to its ability to model complex, non-linear relationships effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9bdcd3-3a3e-4104-b354-80c4f05eff8f",
   "metadata": {},
   "source": [
    "##\n",
    "### 10. What is the effect of the C parameter in SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acfa71-2141-4383-848d-9253225cc223",
   "metadata": {},
   "source": [
    "The **C parameter** in SVM is a **regularization constant** that controls the trade-off between **maximizing the margin** and **minimizing classification errors**.\n",
    "\n",
    "* **High C value:**\n",
    "\n",
    "  * The model prioritizes **correctly classifying all training points**, allowing **smaller margins**.\n",
    "  * Can lead to **overfitting** (less generalization).\n",
    "\n",
    "* **Low C value:**\n",
    "\n",
    "  * Allows **more misclassifications** but aims for a **wider margin**.\n",
    "  * Produces a **simpler, more generalizable model**.\n",
    "\n",
    "👉 In short, **C regulates model flexibility** — large C fits data tightly, small C keeps it smoother and more robust to noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e04f14-ace0-45fa-a739-f9194113ae89",
   "metadata": {},
   "source": [
    "##\n",
    "### 11. What is the role of the Gamma parameter in RBF Kernel SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a2b6c-d373-4abe-a268-a926e82c9253",
   "metadata": {},
   "source": [
    "The **Gamma (γ)** parameter in an **RBF Kernel SVM** defines how far the influence of a single training point reaches — it controls the **curvature** of the decision boundary.\n",
    "\n",
    "* **High γ (large value):**\n",
    "\n",
    "  * Each point has **short-range influence**, leading to **tight, complex boundaries**.\n",
    "  * Model fits training data closely → risk of **overfitting**.\n",
    "\n",
    "* **Low γ (small value):**\n",
    "\n",
    "  * Points have **wider influence**, creating **smoother, simpler boundaries**.\n",
    "  * May lead to **underfitting** if too small.\n",
    "\n",
    "👉 Thus, **Gamma controls the decision region smoothness** — higher values capture detail, while lower values generalize better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52780c5b-761e-4157-92ea-38b91bf42424",
   "metadata": {},
   "source": [
    "##\n",
    "### 12. What is the Naïve Bayes classifier, and why is it called \"Naïve\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b741b-0d0b-4e9f-9a79-573ae5f9f888",
   "metadata": {},
   "source": [
    "**Naïve Bayes** is a **probabilistic classification algorithm** based on **Bayes’ Theorem**, which predicts the probability of a class given the input features. It assumes that all features are **independent of each other**, given the class label.\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{P(X|C) , P(C)}{P(X)}\n",
    "$$\n",
    "\n",
    "It’s called **“Naïve”** because of this **strong independence assumption** — in reality, features are often correlated, but the model still performs remarkably well in many tasks like **spam filtering, sentiment analysis, and text classification**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577dbae-8822-432f-8ecc-96c097faac41",
   "metadata": {},
   "source": [
    "##\n",
    "### 13. What is Bayes’ Theorem ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c75c50-209e-4a9d-bc6f-54067990a850",
   "metadata": {},
   "source": [
    "**Bayes’ Theorem** is a fundamental concept in probability theory that describes how to **update the probability of a hypothesis** based on new evidence.\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) , P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* ( P(A|B) ): Posterior probability — probability of event A given B.\n",
    "* ( P(B|A) ): Likelihood — probability of observing B given A is true.\n",
    "* ( P(A) ): Prior probability of A.\n",
    "* ( P(B) ): Evidence or normalization factor.\n",
    "\n",
    "👉 It allows combining **prior knowledge** with **new data**, forming the foundation of **Bayesian inference** and models like **Naïve Bayes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1507e4d4-5e18-4364-886b-43351bc72942",
   "metadata": {},
   "source": [
    "##\n",
    "### 14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e87ff1-1729-450a-bc0b-44c63be70a2f",
   "metadata": {},
   "source": [
    "| **Type**                    | **Used For**    | **Feature Type**                                     | **Probability Model**         | **Example Use Case**                                                      |                                   |\n",
    "| --------------------------- | --------------- | ---------------------------------------------------- | ----------------------------- | ------------------------------------------------------------------------- | --------------------------------- |\n",
    "| **Gaussian Naïve Bayes**    | Continuous data | Features follow a **normal (Gaussian)** distribution | ( P(x_i                       | y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x_i - \\mu)^2}{2\\sigma^2}} ) | Iris classification, medical data |\n",
    "| **Multinomial Naïve Bayes** | Count data      | Features represent **frequency counts**              | Uses multinomial distribution | Text classification, spam detection                                       |                                   |\n",
    "| **Bernoulli Naïve Bayes**   | Binary data     | Features are **0/1** (present or absent)             | Uses Bernoulli distribution   | Sentiment analysis, document classification                               |                                   |\n",
    "\n",
    "👉 In short:\n",
    "\n",
    "* **Gaussian** → continuous numeric features,\n",
    "* **Multinomial** → word count or frequency data,\n",
    "* **Bernoulli** → binary or Boolean features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bffdc17-23a6-48b1-bc31-fe59f00e7f83",
   "metadata": {},
   "source": [
    "##\n",
    "### 15. When should you use Gaussian Naïve Bayes over other variants ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3953e2-e243-421f-8e62-ae1c1fa83386",
   "metadata": {},
   "source": [
    "You should use **Gaussian Naïve Bayes** when the **features are continuous** and approximately follow a **normal (Gaussian) distribution**.\n",
    "\n",
    "✅ **Best suited for:**\n",
    "\n",
    "* Numerical datasets (e.g., height, weight, age, temperature).\n",
    "* Problems where feature values are **real numbers** rather than counts or binary indicators.\n",
    "* Tasks like **medical diagnosis, sensor data analysis, or image recognition**, where continuous inputs are common.\n",
    "\n",
    "👉 Use **Multinomial NB** for text/count data and **Bernoulli NB** for binary features instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6c29f-e539-475f-94e4-947db4b785a2",
   "metadata": {},
   "source": [
    "##\n",
    "### 16. What are the key assumptions made by Naïve Bayes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51e8f2-e775-4e67-b9b5-cb03a1a86e57",
   "metadata": {},
   "source": [
    "The **key assumptions** of the Naïve Bayes algorithm are:\n",
    "\n",
    "1. **Feature Independence:**\n",
    "   All features are assumed to be **independent** of each other given the class label.\n",
    "\n",
    "2. **Equal Importance of Features:**\n",
    "   Each feature contributes **equally and independently** to the outcome.\n",
    "\n",
    "3. **Conditional Probability Validity:**\n",
    "   The probability of each feature given the class (( P(X_i|Y) )) is estimated correctly from the data.\n",
    "\n",
    "4. **Distributional Assumption (Variant-specific):**\n",
    "\n",
    "   * **Gaussian NB:** Features follow a **normal distribution**.\n",
    "   * **Multinomial NB:** Features represent **counts/frequencies**.\n",
    "   * **Bernoulli NB:** Features are **binary (0/1)**.\n",
    "\n",
    "👉 These assumptions simplify computation and make Naïve Bayes **fast, efficient, and effective**, even when the independence assumption is not fully true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b0992-dae5-4e86-bd94-40bbc52d17c5",
   "metadata": {},
   "source": [
    "##\n",
    "### 17. What are the advantages and disadvantages of Naïve Bayes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5272dc9-99ec-43b6-a6a8-eb3be55881dd",
   "metadata": {},
   "source": [
    "**Advantages:**\n",
    "\n",
    "* ✅ **Fast and efficient** — simple to implement and computationally inexpensive.\n",
    "* ✅ **Performs well on small datasets** and in **high-dimensional spaces** (e.g., text data).\n",
    "* ✅ Works surprisingly well even when the **independence assumption** is partially violated.\n",
    "* ✅ Requires **less training data** and handles **multiclass classification** effectively.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "* ❌ Assumes **feature independence**, which rarely holds true in real data.\n",
    "* ❌ Performs poorly when features are **highly correlated**.\n",
    "* ❌ Struggles with **zero probabilities** (if a feature value never appears in training data).\n",
    "* ❌ Continuous data must fit the **assumed distribution** (e.g., Gaussian).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c70a3a-5cfc-4d9b-80f4-ada4b28647c2",
   "metadata": {},
   "source": [
    "##\n",
    "### 18. Why is Naïve Bayes a good choice for text classification ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a98f2-9330-4510-abfc-d1ad16282367",
   "metadata": {},
   "source": [
    "**Naïve Bayes** is an excellent choice for **text classification** because:\n",
    "\n",
    "* ✅ **Feature independence** assumption fits naturally since words (tokens) are often treated as independent.\n",
    "* ✅ Handles **high-dimensional sparse data** efficiently — common in text (e.g., thousands of words per document).\n",
    "* ✅ **Fast training and prediction**, even on large corpora.\n",
    "* ✅ Works well with **word count or frequency data**, especially with **Multinomial Naïve Bayes**.\n",
    "* ✅ Performs robustly in **spam detection, sentiment analysis, and topic classification** with minimal preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672b201-ed55-4d19-9e11-7795cea12242",
   "metadata": {},
   "source": [
    "##\n",
    "### 19. Compare SVM and Naïve Bayes for classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9700e4-fe55-48ba-8a0c-c248b42378a0",
   "metadata": {},
   "source": [
    "| **Aspect**               | **Support Vector Machine (SVM)**                   | **Naïve Bayes (NB)**                              |\n",
    "| ------------------------ | -------------------------------------------------- | ------------------------------------------------- |\n",
    "| **Model Type**           | Discriminative (finds boundary between classes)    | Generative (models class probabilities)           |\n",
    "| **Data Assumption**      | No specific distribution assumption                | Assumes feature independence                      |\n",
    "| **Computation**          | Computationally heavier, slower on large data      | Very fast and efficient                           |\n",
    "| **Performance**          | High accuracy on complex and high-dimensional data | Performs well on simple or text-based data        |\n",
    "| **Interpretability**     | Harder to interpret                                | Easy to interpret with clear probabilistic output |\n",
    "| **Best For**             | Non-linear, complex decision boundaries            | Text, spam, or sentiment classification           |\n",
    "| **Sensitivity to Noise** | Robust with proper tuning (C, γ)                   | Sensitive to correlated or noisy features         |\n",
    "\n",
    "👉 In summary, **SVM** is more powerful for **complex boundaries**, while **Naïve Bayes** is preferred for **speed, simplicity, and text data**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc0ea8-f708-4ad9-a8c1-d7d10cf51701",
   "metadata": {},
   "source": [
    "##\n",
    "### 20. How does Laplace Smoothing help in Naïve Bayes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c344e4bd-9191-4977-87e2-24e6351324fa",
   "metadata": {},
   "source": [
    "**Laplace Smoothing** (also called **Additive Smoothing**) prevents zero probabilities in Naïve Bayes when a word or feature doesn’t appear in the training data for a given class.\n",
    "\n",
    "It modifies the probability formula as:\n",
    "\n",
    "$$\n",
    "P(x_i|y) = \\frac{count(x_i, y) + 1}{count(y) + n}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* ( count(x_i, y) ) = frequency of feature ( x_i ) in class ( y )\n",
    "* ( n ) = total number of unique features\n",
    "\n",
    "✅ **Benefits:**\n",
    "\n",
    "* Avoids **zero probability errors** that would make the entire class probability zero.\n",
    "* Ensures **better generalization** for unseen data during prediction.\n",
    "\n",
    "👉 Especially useful in **text classification**, where new or rare words often appear in test data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
