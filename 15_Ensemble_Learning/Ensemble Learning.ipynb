{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "258f9f31-8a2b-45e2-89d6-da448e96c7d7",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef5f85-dd31-46d1-8dd1-feb2e47fe89b",
   "metadata": {},
   "source": [
    "##\n",
    "### Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623dd4d-8f1c-419e-9d24-457a129e69b4",
   "metadata": {},
   "source": [
    "**Ensemble Learning** is a machine learning technique that combines **multiple models (weak learners)** to create a **stronger, more accurate predictive model**.\n",
    "\n",
    "* The key idea is that **aggregating diverse models** (e.g., Decision Trees, SVMs) helps reduce **variance, bias, and overfitting**, improving overall performance.\n",
    "* Common ensemble methods include **Bagging**, **Boosting**, and **Stacking**, each using different strategies to combine model outputs for better generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7db38-c394-4a76-9f78-9678985e7382",
   "metadata": {},
   "source": [
    "##\n",
    "### Question 2: What is the difference between Bagging and Boosting ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f6be3-e1f8-423b-8a68-20aad0becf9e",
   "metadata": {},
   "source": [
    "| **Aspect**             | **Bagging (Bootstrap Aggregating)**                           | **Boosting**                                                                   |\n",
    "| ---------------------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------ |\n",
    "| **Objective**          | Reduce variance and prevent overfitting                       | Reduce bias and improve weak learners                                          |\n",
    "| **Model Training**     | Models are trained **independently in parallel**              | Models are trained **sequentially**, each correcting the previous oneâ€™s errors |\n",
    "| **Data Sampling**      | Uses **random sampling with replacement** (bootstrap samples) | Each new model focuses on **misclassified samples** from previous rounds       |\n",
    "| **Example Algorithms** | Random Forest, Bagged Trees                                   | AdaBoost, Gradient Boosting, XGBoost                                           |\n",
    "| **Error Handling**     | Averages predictions to smooth noise                          | Assigns **higher weights** to difficult cases                                  |\n",
    "| **Bias & Variance**    | Reduces **variance**                                          | Reduces **bias** (and sometimes variance)                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aeb03fe-12e8-443e-9673-8e3e490639a7",
   "metadata": {},
   "source": [
    "##\n",
    "### Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b70f1-2340-4fcc-875b-5c59401d14ba",
   "metadata": {},
   "source": [
    "**Bootstrap sampling** is a statistical technique where **random samples are drawn with replacement** from the original dataset to create multiple new datasets (bootstrap samples) of the same size.\n",
    "\n",
    "In **Bagging methods** like **Random Forest**, bootstrap sampling ensures:\n",
    "\n",
    "* Each base model (e.g., Decision Tree) is trained on a **different subset** of data.\n",
    "* Introduces **diversity among models**, reducing correlation between them.\n",
    "* Helps in **reducing overfitting** and improving **model stability and generalization**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6198402-9411-47ed-a4c6-e138e02dcfca",
   "metadata": {},
   "source": [
    "##\n",
    "### Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85b0ec-d78f-4449-b368-d33a9664b36f",
   "metadata": {},
   "source": [
    "**Out-of-Bag (OOB) samples** are the data points **not included** in a bootstrap sample during model training in Bagging methods like Random Forest. Typically, about **one-third of the data** remains OOB for each model.\n",
    "\n",
    "The **OOB score** evaluates model performance by:\n",
    "\n",
    "* Using these OOB samples as a **validation set** for each base learner.\n",
    "* Computing the **average accuracy (or error)** across all OOB predictions.\n",
    "* Providing a **built-in, unbiased estimate** of model performance **without needing a separate test set**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dec301-3a45-4166-8580-afc9f81cd96c",
   "metadata": {},
   "source": [
    "##\n",
    "### Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccb7d74-2058-43f3-a392-51471699c1ab",
   "metadata": {},
   "source": [
    "| **Aspect**            | **Decision Tree**                                                          | **Random Forest**                                                               |\n",
    "| --------------------- | -------------------------------------------------------------------------- | ------------------------------------------------------------------------------- |\n",
    "| **Computation Basis** | Based on **information gain** or **Gini reduction** from individual splits | Averaged **feature importance scores** across all trees in the ensemble         |\n",
    "| **Interpretability**  | Easier to interpret â€” clear feature ranking from one tree                  | Harder to interpret â€” aggregated importance across many trees                   |\n",
    "| **Stability**         | Can vary greatly with small data changes                                   | More **stable and reliable** due to averaging over multiple trees               |\n",
    "| **Bias**              | May overemphasize dominant features                                        | Provides **balanced feature importance** by reducing bias from individual trees |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02002760-f28a-4c37-97a9-2d8ba46fc707",
   "metadata": {},
   "source": [
    "##\n",
    "### Question 6: Write a Python program to:\n",
    "* Load the Breast Cancer dataset using\n",
    "sklearn.datasets.load_breast_cancer()\n",
    "* Train a Random Forest Classifier\n",
    "* Print the top 5 most important features based on feature importance scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5a244-1d53-4051-8375-af9a5072f941",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39f4f227-972f-4fb8-aa05-1b2b541e11e6",
   "metadata": {},
   "source": [
    "##\n",
    "### Question 7: Write a Python program to:\n",
    "* Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
    "* Evaluate its accuracy and compare with a single Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d285ddb-3b60-4203-985a-02249e2db14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Most Important Features:\n",
      "                 Feature  Importance\n",
      "23            worst area    0.153892\n",
      "27  worst concave points    0.144663\n",
      "7    mean concave points    0.106210\n",
      "20          worst radius    0.077987\n",
      "6         mean concavity    0.068001\n"
     ]
    }
   ],
   "source": [
    "# --- Import Libraries ---\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# --- Load Dataset ---\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target\n",
    "\n",
    "# --- Split Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Train Random Forest Classifier ---\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Compute Feature Importances ---\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# --- Display Top 5 Important Features ---\n",
    "print(\"Top 5 Most Important Features:\")\n",
    "print(feature_importances.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b91027-09b2-4d6a-859b-4cb2c7eb445f",
   "metadata": {},
   "source": [
    "##\n",
    "### Question 8: Write a Python program to:\n",
    "* Train a Random Forest Classifier\n",
    "* Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
    "* Print the best parameters and final accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf13f0b-fc6e-43ca-957b-7d4363e18c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'n_estimators': 150}\n",
      "Final Model Accuracy: 0.9649\n"
     ]
    }
   ],
   "source": [
    "# --- Import Libraries ---\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Load Dataset ---\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# --- Split Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Define Random Forest Classifier ---\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# --- Define Parameter Grid ---\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 7, None]\n",
    "}\n",
    "\n",
    "# --- Apply GridSearchCV ---\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
    "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# --- Best Parameters and Final Model ---\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# --- Evaluate Accuracy ---\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(f\"Final Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef87c97d-973a-4e75-89ec-9029c0525d4f",
   "metadata": {},
   "source": [
    "##\n",
    "### Question 9: Write a Python program to:\n",
    "* Train a Bagging Regressor and a Random Forest Regressor on the California\n",
    "Housing dataset\n",
    "* Compare their Mean Squared Errors (MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c0c2b0e-010b-47b9-9f09-7e7b4f757f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor MSE: 0.3312\n",
      "Random Forest Regressor MSE: 0.3315\n"
     ]
    }
   ],
   "source": [
    "# --- Import Libraries ---\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# --- Load Dataset ---\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# --- Use a Subset for Faster Computation ---\n",
    "np.random.seed(42)\n",
    "subset_idx = np.random.choice(len(X), 2000, replace=False)\n",
    "X, y = X[subset_idx], y[subset_idx]\n",
    "\n",
    "# --- Split Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Train Bagging Regressor (Base: Decision Tree) ---\n",
    "bagging_model = BaggingRegressor(\n",
    "    estimator=DecisionTreeRegressor(), \n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "bagging_model.fit(X_train, y_train)\n",
    "bagging_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# --- Train Random Forest Regressor ---\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# --- Compute Mean Squared Errors ---\n",
    "mse_bagging = mean_squared_error(y_test, bagging_pred)\n",
    "mse_rf = mean_squared_error(y_test, rf_pred)\n",
    "\n",
    "# --- Print Comparison ---\n",
    "print(f\"Bagging Regressor MSE: {mse_bagging:.4f}\")\n",
    "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e017c-d5f4-485d-9189-e3ab32f5cc27",
   "metadata": {},
   "source": [
    "##\n",
    "### Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
    "You decide to use ensemble techniques to increase model performance.\n",
    "Explain your step-by-step approach to:\n",
    "* Choose between Bagging or Boosting\n",
    "* Handle overfitting\n",
    "* Select base models\n",
    "* Evaluate performance using cross-validation\n",
    "* Justify how ensemble learning improves decision-making in this real-world\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f6a538-3959-4312-aa13-acff1a49e989",
   "metadata": {},
   "source": [
    "\n",
    "### **Step-by-Step Approach: Predicting Loan Default Using Ensemble Learning**\n",
    "\n",
    "#### **1. Choosing Between Bagging and Boosting**\n",
    "\n",
    "* **Bagging** (e.g., Random Forest) is ideal when the goal is to **reduce variance** and handle noisy data.\n",
    "* **Boosting** (e.g., XGBoost, AdaBoost) is preferred when you need to **reduce bias** and improve weak learners.\n",
    "  ðŸ‘‰ For loan default prediction (imbalanced and complex), **Boosting** is generally better as it focuses on **hard-to-predict cases**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Handling Overfitting**\n",
    "\n",
    "* Use **cross-validation** and **early stopping** (for boosting models like XGBoost).\n",
    "* Tune parameters such as:\n",
    "\n",
    "  * `max_depth` (shallower trees prevent overfitting)\n",
    "  * `learning_rate` (small values like 0.05â€“0.1)\n",
    "  * `n_estimators` (balance model complexity and accuracy)\n",
    "* Apply **regularization** (`lambda`, `alpha`) and **dropout** techniques in boosting.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Selecting Base Models**\n",
    "\n",
    "* Start with **Decision Trees** as base learners (simple and diverse).\n",
    "* For experimentation, combine different learners (e.g., Logistic Regression + Tree + SVM) in **Stacking** for better diversity.\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Evaluating Performance**\n",
    "\n",
    "* Use **Stratified K-Fold Cross-Validation** to ensure balanced class representation.\n",
    "* Evaluate metrics like:\n",
    "\n",
    "  * **ROC-AUC** (model discrimination power)\n",
    "  * **Precision, Recall, F1-score** (important for imbalanced data)\n",
    "  * **Confusion Matrix** for interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. Business Justification**\n",
    "\n",
    "* Ensemble models **increase prediction reliability** by aggregating multiple perspectives.\n",
    "* In a financial context, better predictions mean:\n",
    "\n",
    "  * **Reduced default risk**\n",
    "  * **Improved credit decision-making**\n",
    "  * **Higher profitability and lower losses**\n",
    "* The model aids in **trustworthy, data-driven loan approvals** while minimizing false approvals or rejections.\n",
    "\n",
    "---\n",
    "\n",
    "âœ… **In summary:**\n",
    "Use **Boosting (e.g., XGBoost)** with strong regularization, cross-validation, and interpretability tools (e.g., SHAP) to build a **robust, transparent, and business-impactful loan default prediction system**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
