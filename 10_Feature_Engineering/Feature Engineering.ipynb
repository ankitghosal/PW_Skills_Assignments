{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9edb2587-a3e5-4337-b1ae-bb235d1e9d61",
   "metadata": {},
   "source": [
    "###\n",
    "### 1.  What is a parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c2c58-8074-443f-9232-df6ac9531772",
   "metadata": {},
   "source": [
    "A **parameter** is a numerical value that describes a characteristic of a **population** in statistics. It is fixed but usually unknown, as it represents the true value for the entire population.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "* The average height of all adults in a country (true population mean, Î¼) is a parameter.\n",
    "* In practice, we estimate it using a **statistic** (like the sample mean, xÌ„).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788dcc82-cc8a-4c10-9b0a-cccfd41b86dc",
   "metadata": {},
   "source": [
    "####\n",
    "### 2. What is correlation? What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6800d0e0-e310-4722-b6f0-434e60fed847",
   "metadata": {},
   "source": [
    "**Correlation** is a statistical measure that indicates the strength and direction of a relationship between two variables, ranging from **-1 to +1**.\n",
    "\n",
    "* A **positive correlation** means that as one variable increases, the other tends to increase.\n",
    "* A **negative correlation** means that as one variable increases, the other tends to decrease (inverse relationship).\n",
    "\n",
    "**Example:** Hours studied â†‘ vs. errors made in an exam â†“ â†’ negative correlation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a1c6d-e4e8-40b9-a586-b9cae89d86a9",
   "metadata": {},
   "source": [
    "####\n",
    "### 3. Define Machine Learning. What are the main components in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a59d89-4782-479f-9fce-3f457d458040",
   "metadata": {},
   "source": [
    "**Machine Learning (ML)** is a subset of artificial intelligence where systems learn patterns from data and improve their performance on tasks **without being explicitly programmed**.\n",
    "\n",
    "**Main components of ML:**\n",
    "\n",
    "* **Dataset:** Collection of input data used for training and testing.\n",
    "* **Features:** Independent variables or attributes used for prediction.\n",
    "* **Model/Algorithm:** Mathematical method (e.g., regression, decision tree) that learns from data.\n",
    "* **Training Process:** Feeding data to the model to adjust parameters.\n",
    "* **Evaluation:** Measuring performance using metrics (e.g., accuracy, precision).\n",
    "* **Prediction:** Applying the trained model to unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a97cdb-f410-4569-8309-b2a589d06a46",
   "metadata": {},
   "source": [
    "####\n",
    "### 4. How does loss value help in determining whether the model is good or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3fef0-8174-4bb5-89f7-b04b3a9f7e95",
   "metadata": {},
   "source": [
    "The **loss value** measures how far a modelâ€™s predictions are from the actual values â€” it quantifies the **error** in the model.\n",
    "\n",
    "* A **low loss value** indicates that predictions are close to the true outputs â†’ the model is performing well.\n",
    "* A **high loss value** suggests large errors â†’ the model is not learning effectively.\n",
    "* Tracking loss during training helps in monitoring whether the model is improving, overfitting, or underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0b9e75-7816-446c-987d-0850961338b0",
   "metadata": {},
   "source": [
    "####\n",
    "### 5. What are continuous and categorical variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cebb765-3528-49ec-b585-ae796544a2b2",
   "metadata": {},
   "source": [
    "* **Continuous Variables:** Numerical variables that can take an **infinite number of values within a range**. They are measurable.\n",
    "\n",
    "  * *Examples:* height, weight, temperature, income.\n",
    "\n",
    "* **Categorical Variables:** Variables that represent **distinct categories or groups** with no inherent numerical meaning.\n",
    "\n",
    "  * *Examples:* gender, blood type, car brand, marital status.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3db941e-56b4-4f3d-8996-303e4de8e0cd",
   "metadata": {},
   "source": [
    "####\n",
    "### 6. How do we handle categorical variables in Machine Learning? What are the common techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44df77d-ef8f-4fb6-81d7-ea990fc0a762",
   "metadata": {},
   "source": [
    "Categorical variables must be converted into a numerical format for most ML algorithms. Common techniques include:\n",
    "\n",
    "* **Label Encoding:** Assigns each category a unique integer (e.g., `Male=0, Female=1`).\n",
    "* **One-Hot Encoding:** Creates binary columns for each category (e.g., `Red=[1,0,0], Blue=[0,1,0]`).\n",
    "* **Ordinal Encoding:** Assigns ordered integers when categories have a natural order (e.g., `Low=1, Medium=2, High=3`).\n",
    "* **Target Encoding:** Replaces categories with statistical measures (e.g., mean of target variable for that category).\n",
    "\n",
    "ðŸ‘‰ Choice depends on whether the variable is **nominal** (no order) or **ordinal** (with order).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c1031b-aaa1-430b-a2de-92996d3705b1",
   "metadata": {},
   "source": [
    "####\n",
    "### 7. What do you mean by training and testing a dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04249da-63b1-4a43-b7e5-57d3c27c8480",
   "metadata": {},
   "source": [
    "* **Training Dataset:** A portion of the data used to **teach the model** by adjusting its parameters so it can learn patterns and relationships.\n",
    "* **Testing Dataset:** A separate portion of the data used **after training** to evaluate how well the model generalizes to unseen data.\n",
    "\n",
    "ðŸ‘‰ Splitting ensures the model is not just memorizing but can perform accurately on new, real-world data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5250d0-8965-4e6c-a4a6-09700b569f3d",
   "metadata": {},
   "source": [
    "####\n",
    "### 8. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663a46c-3d0d-4d32-a654-37b1a9c7314b",
   "metadata": {},
   "source": [
    "**`sklearn.preprocessing`** is a module in **Scikit-learn** that provides tools to **transform and scale data** before feeding it into machine learning models.\n",
    "\n",
    "It includes methods for:\n",
    "\n",
    "* **Scaling & Normalization:** `StandardScaler`, `MinMaxScaler`.\n",
    "* **Encoding Categorical Variables:** `LabelEncoder`, `OneHotEncoder`.\n",
    "* **Generating Polynomial Features:** `PolynomialFeatures`.\n",
    "* **Handling Missing Values / Imputation:** `SimpleImputer`.\n",
    "\n",
    "ðŸ‘‰ These preprocessing steps ensure data is in the right format and scale for better model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c143d3d8-7e8b-4589-9870-ca8fe7dde49f",
   "metadata": {},
   "source": [
    "####\n",
    "### 9. What is a Test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a425d1-14c9-4841-bef8-1826e91d82a7",
   "metadata": {},
   "source": [
    "A **Test set** is a subset of the dataset that is **kept aside during training** and used only to **evaluate the final modelâ€™s performance**.\n",
    "\n",
    "* It represents **unseen data** to check how well the model generalizes.\n",
    "* Metrics like **accuracy, precision, recall, F1-score, RMSE** are calculated on the test set.\n",
    "* Helps prevent overfitting by ensuring the model works beyond the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8ededa-8625-4a54-ad2c-5f9af1e06075",
   "metadata": {},
   "source": [
    "####\n",
    "### 10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c44f6-811a-4bf8-a3b6-4b36e0853d3b",
   "metadata": {},
   "source": [
    "**Splitting Data in Python:**\n",
    "We typically use **`train_test_split`** from Scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "* `test_size=0.2` â†’ 20% data for testing, 80% for training.\n",
    "* `random_state` ensures reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "**Approach to a Machine Learning Problem:**\n",
    "\n",
    "1. **Understand the Problem:** Define objectives and target variable.\n",
    "2. **Collect & Explore Data:** Perform EDA (Exploratory Data Analysis).\n",
    "3. **Preprocess Data:** Handle missing values, encode categories, scale features.\n",
    "4. **Split Data:** Train/Test (and sometimes validation) split.\n",
    "5. **Select Model:** Choose algorithm(s) suitable for the problem.\n",
    "6. **Train the Model:** Fit on training data.\n",
    "7. **Evaluate Performance:** Test using metrics (accuracy, RMSE, etc.).\n",
    "8. **Tune Hyperparameters:** Optimize model performance.\n",
    "9. **Deploy & Monitor:** Put the model into production and track results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f93ac-2b71-44e3-8ebc-f3c5dda1de78",
   "metadata": {},
   "source": [
    "####\n",
    "### 11. Why do we have to perform EDA before fitting a model to the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f6894b-40ea-4ef1-bfb9-7ff391dc2e4b",
   "metadata": {},
   "source": [
    "**`Exploratory Data Analysis (EDA)`** is the process of **analyzing and visualizing data** to understand its structure, patterns, and anomalies before applying machine learning models.\n",
    "\n",
    "It helps in:\n",
    "\n",
    "* **Identifying Missing Values & Outliers:** Detects gaps or extreme values that can affect model performance.\n",
    "* **Understanding Data Distributions:** Reveals how features and the target variable are distributed.\n",
    "* **Detecting Relationships & Correlations:** Highlights dependencies between features which may inform feature selection.\n",
    "* **Assessing Class Imbalance:** Checks if the target variable has skewed classes that need handling.\n",
    "\n",
    "ðŸ‘‰ Performing EDA ensures the data is **clean, consistent, and properly structured**, which improves model accuracy and reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a89c164-0324-4b28-8f2f-4f162b9de5db",
   "metadata": {},
   "source": [
    "####\n",
    "### 12. What is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6216c7-d2cf-47c6-8345-4e27ba8e8773",
   "metadata": {},
   "source": [
    "**`Correlation`** is a statistical measure that describes the **strength and direction of a relationship** between two variables.\n",
    "\n",
    "It helps in understanding how one variable **changes in relation to another**.\n",
    "\n",
    "Key points:\n",
    "\n",
    "* **Positive Correlation:** Both variables increase or decrease together.\n",
    "* **Negative Correlation:** One variable increases while the other decreases.\n",
    "* **No Correlation:** No predictable relationship between the variables.\n",
    "* **Common Measure:** `Pearson correlation coefficient` (ranges from -1 to +1).\n",
    "\n",
    "ðŸ‘‰ Correlation is useful in **feature selection**, identifying redundant variables, and understanding patterns in the dataset before modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d42eee1-8c2f-4756-a4c4-8d6155cb2993",
   "metadata": {},
   "source": [
    "####\n",
    "### 13. What does negative correlation mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d609c025-ece2-45e6-9d97-6800a537fddb",
   "metadata": {},
   "source": [
    "**`Negative Correlation`** refers to a relationship between two variables where **as one variable increases, the other decreases**, and vice versa.\n",
    "\n",
    "Key points:\n",
    "\n",
    "* **Direction:** Inversely proportional â€“ when one goes up, the other goes down.\n",
    "* **Strength:** Measured by the correlation coefficient (ranges from -1 to 0 for negative correlation).\n",
    "* **Example:** As temperature rises, heating energy consumption usually decreases.\n",
    "* **Use in ML:** Helps identify features that move in opposite directions, which can inform feature selection and model interpretation.\n",
    "\n",
    "ðŸ‘‰ Negative correlation indicates an **inverse relationship** between variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a2b580-898a-445f-9e25-d5b8ed761328",
   "metadata": {},
   "source": [
    "####\n",
    "### 14. How can you find correlation between variables in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9941e9-03f5-4dce-ab26-3d8f72f6d089",
   "metadata": {},
   "source": [
    "**Finding Correlation Between Variables in Python** can be done using **pandas** and **visualization libraries** like **seaborn**.\n",
    "\n",
    "Common methods:\n",
    "\n",
    "* **Using `pandas.DataFrame.corr()`:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)\n",
    "```\n",
    "\n",
    "This computes pairwise correlation (Pearson by default) between numerical features.\n",
    "\n",
    "* **Visualizing with a Heatmap (Seaborn):**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Heatmaps make it easier to spot strong positive or negative correlations.\n",
    "\n",
    "* **Other Correlation Methods:** `df.corr(method='spearman')` or `method='kendall'` for non-linear or rank-based relationships.\n",
    "\n",
    "ðŸ‘‰ These techniques help identify **relationships between features** and guide feature selection or engineering for machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d95add-ab52-40c0-8869-90b743220aa1",
   "metadata": {},
   "source": [
    "####\n",
    "### 15. What is causation? Explain difference between correlation and causation with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b473ed0-9f16-4dc7-a1f2-8f9c2cd82a68",
   "metadata": {},
   "source": [
    "**`Causation`** (or causal relationship) occurs when a change in one variable **directly causes a change** in another variable.\n",
    "\n",
    "Key points:\n",
    "\n",
    "* **Direction of Influence:** One variable directly affects the other.\n",
    "* **Requires Evidence:** Usually determined through experiments or controlled studies, not just observation.\n",
    "\n",
    "**Difference between Correlation and Causation:**\n",
    "\n",
    "| Aspect           | Correlation                                  | Causation                                 |\n",
    "| ---------------- | -------------------------------------------- | ----------------------------------------- |\n",
    "| **Definition**   | Measures how two variables move together     | One variable **directly affects** another |\n",
    "| **Relationship** | May be direct, inverse, or even coincidental | Must be a direct cause-effect link        |\n",
    "| **Example**      | Ice cream sales â†‘ and drowning incidents â†‘   | Heating the room â†‘ â†’ Room temperature â†‘   |\n",
    "| **Implication**  | Does not imply one causes the other          | Implies a causal effect                   |\n",
    "\n",
    "ðŸ‘‰ **Important:** Correlation does **not imply causation**. Two variables can be correlated due to coincidence or a third confounding factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e7849-2597-452e-bbef-4a7f6920d412",
   "metadata": {},
   "source": [
    "####\n",
    "### 16. What is an Optimizer? What are different types of optimizers? Explain each with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb45bcf7-5f86-4697-a1ed-95f22eff903e",
   "metadata": {},
   "source": [
    "**`Optimizer`** is an algorithm used in **machine learning and deep learning** to **adjust the modelâ€™s parameters (weights and biases) during training** in order to **minimize the loss function** and improve model performance.\n",
    "\n",
    "Optimizers control **how the model learns** and can significantly affect **training speed and accuracy**.\n",
    "\n",
    "**Common Types of Optimizers:**\n",
    "\n",
    "* **1. Gradient Descent (GD):**\n",
    "  Updates weights using the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "  * **Batch Gradient Descent:** Uses the entire dataset for one update.\n",
    "\n",
    "    ```python\n",
    "    # Conceptual\n",
    "    weights = weights - learning_rate * gradient(loss)\n",
    "    ```\n",
    "  * **Example:** Training a linear regression model on a small dataset.\n",
    "\n",
    "* **2. Stochastic Gradient Descent (SGD):**\n",
    "  Updates weights for **each training sample**, making it faster but more noisy.\n",
    "\n",
    "  * **Example:** Large datasets where full batch computation is costly.\n",
    "\n",
    "* **3. Mini-batch Gradient Descent:**\n",
    "  Compromise between GD and SGD; updates weights using small batches of data.\n",
    "\n",
    "  * **Example:** Commonly used in deep learning (batch size = 32, 64, etc.).\n",
    "\n",
    "* **4. Momentum:**\n",
    "  Accelerates gradient descent by considering **past updates** to reduce oscillations.\n",
    "\n",
    "  * **Example:** Helps speed up training in neural networks with steep or flat regions.\n",
    "\n",
    "* **5. Adaptive Methods:**\n",
    "\n",
    "  * **Adagrad:** Adjusts learning rate per parameter based on historical gradients.\n",
    "  * **RMSprop:** Modifies Adagrad to work well in non-stationary settings.\n",
    "  * **Adam:** Combines Momentum and RMSprop for fast and efficient training.\n",
    "\n",
    "    ```python\n",
    "    # Using Adam in TensorFlow/Keras\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    ```\n",
    "\n",
    "ðŸ‘‰ Optimizers are **crucial** for efficiently finding the best model parameters and ensuring **faster convergence** during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc54a1b-1722-4df3-8821-f3811b3aaf2e",
   "metadata": {},
   "source": [
    "####\n",
    "### 17. What is sklearn.linear_model ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737f0e8-73b6-4761-bfd5-1ff838393cbd",
   "metadata": {},
   "source": [
    "**`sklearn.linear_model`** is a module in **Scikit-learn** that provides a collection of **linear models** for regression and classification tasks.\n",
    "\n",
    "It includes algorithms that assume a **linear relationship between input features and the target variable**.\n",
    "\n",
    "Key features and classes:\n",
    "\n",
    "* **Linear Regression:** `LinearRegression()` â€“ Fits a line to predict continuous values.\n",
    "* **Logistic Regression:** `LogisticRegression()` â€“ Used for binary or multiclass classification.\n",
    "* **Ridge & Lasso Regression:** `Ridge()`, `Lasso()` â€“ Linear regression with **L2 or L1 regularization** to prevent overfitting.\n",
    "* **ElasticNet:** `ElasticNet()` â€“ Combines L1 and L2 regularization.\n",
    "* **SGDRegressor / SGDClassifier:** Implements **stochastic gradient descent** for linear models.\n",
    "\n",
    "ðŸ‘‰ This module is widely used for **predictive modeling** when relationships are approximately linear and provides tools for **regularization, feature selection, and efficient optimization**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112a9cb7-74b5-4644-aa96-64ea8bb1f97b",
   "metadata": {},
   "source": [
    "####\n",
    "### 18. What does model.fit() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f4e691-4534-48f6-bd0c-9ece36a85966",
   "metadata": {},
   "source": [
    "**`model.fit()`** is a method in **Scikit-learn** used to **train a machine learning model** on a given dataset. It **learns the relationships between input features and the target variable** by adjusting the modelâ€™s parameters.\n",
    "\n",
    "Key points:\n",
    "\n",
    "* **Purpose:** Fits the model to the training data so it can make predictions on new data.\n",
    "* **Common Arguments:**\n",
    "\n",
    "  * `X` â†’ Input features (2D array or DataFrame)\n",
    "  * `y` â†’ Target variable (1D array, Series, or DataFrame)\n",
    "  * Optional parameters may include `sample_weight` or other model-specific options\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)  # X_train = features, y_train = target\n",
    "```\n",
    "\n",
    "ðŸ‘‰ After `fit()`, the model has **learned the optimal parameters** (e.g., weights in linear regression) and is ready for prediction with `model.predict()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8a42af-ca03-4586-ae5c-f9c262a9365d",
   "metadata": {},
   "source": [
    "####\n",
    "### 19. What does model.predict() do? What arguments must be given?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c6fe70-7687-4e0d-9f90-a363acf60cd4",
   "metadata": {},
   "source": [
    "**`model.predict()`** is a method in **Scikit-learn** used to **make predictions using a trained machine learning model**.\n",
    "\n",
    "Key points:\n",
    "\n",
    "* **Purpose:** Uses the modelâ€™s learned parameters (from `model.fit()`) to predict the target variable for new input data.\n",
    "* **Common Arguments:**\n",
    "\n",
    "  * `X` â†’ Input features (2D array or DataFrame) for which predictions are required\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Assuming model is already trained\n",
    "predictions = model.predict(X_test)  # X_test = new input features\n",
    "```\n",
    "\n",
    "ðŸ‘‰ The output is an **array of predicted values** (continuous for regression, class labels for classification) based on the input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11073d4a-1f61-4ce3-a1cb-8ba90e5310f0",
   "metadata": {},
   "source": [
    "####\n",
    "### 20. What are continuous and categorical variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2265f-3a0b-4acd-9212-e78de68fef3a",
   "metadata": {},
   "source": [
    "**Variables** in a dataset are generally classified into **continuous** and **categorical** based on the type of values they hold.\n",
    "\n",
    "* **Continuous Variables:**\n",
    "\n",
    "  * Take **numerical values** that can vary **infinitely within a range**.\n",
    "  * Can be **measured** and often allow decimal points.\n",
    "  * **Examples:** Age, Height, Weight, Temperature, Income.\n",
    "\n",
    "* **Categorical Variables:**\n",
    "\n",
    "  * Take **discrete values** representing **categories or groups**.\n",
    "  * Often **non-numeric** or encoded as numbers for modeling.\n",
    "  * **Examples:** Gender (Male/Female), Color (Red/Blue/Green), Payment Type (Cash/Card).\n",
    "\n",
    "ðŸ‘‰ Correctly identifying these variables is important for **data preprocessing, feature encoding, and choosing the right ML algorithms**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73334a52-90d9-4684-bc6b-4628dab47f3d",
   "metadata": {},
   "source": [
    "####\n",
    "### 21. What is feature scaling? How does it help in Machine Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d8294-2435-4699-882a-2aba9ea4220d",
   "metadata": {},
   "source": [
    "**`Feature Scaling`** is the process of **normalizing or standardizing the range of independent variables (features)** in a dataset so that they have comparable scales.\n",
    "\n",
    "Key points:\n",
    "\n",
    "* **Purpose:** Ensures that **all features contribute equally** to the model and prevents features with larger ranges from dominating.\n",
    "* **Common Techniques:**\n",
    "\n",
    "  * **Standardization:** `StandardScaler()` â†’ scales features to have **mean = 0** and **standard deviation = 1**\n",
    "  * **Normalization:** `MinMaxScaler()` â†’ scales features to a **range \\[0, 1]**\n",
    "* **Importance in ML:**\n",
    "\n",
    "  * Improves **convergence speed** for gradient-based algorithms (e.g., **Gradient Descent, Neural Networks**)\n",
    "  * Helps algorithms that rely on **distance metrics** (e.g., **KNN, K-Means, SVM**) perform better\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "ðŸ‘‰ Feature scaling ensures the model **learns efficiently and avoids bias toward features with larger magnitudes**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef28c96-d778-43fb-9f9e-51e941df323c",
   "metadata": {},
   "source": [
    "####\n",
    "### 22. How do we perform scaling in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f05c5ef-d12a-4ecc-9193-7701363ccf43",
   "metadata": {},
   "source": [
    "**`Feature Scaling in Python`** is typically done using **Scikit-learnâ€™s preprocessing module**.\n",
    "\n",
    "Common methods:\n",
    "\n",
    "* **1. Standardization (Z-score Scaling):**\n",
    "  Scales features to have **mean = 0** and **standard deviation = 1**.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "  scaler = StandardScaler()\n",
    "  X_scaled = scaler.fit_transform(X)  # X = input features\n",
    "  ```\n",
    "\n",
    "* **2. Normalization (Min-Max Scaling):**\n",
    "  Scales features to a **specific range**, usually `[0, 1]`.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "  scaler = MinMaxScaler()\n",
    "  X_scaled = scaler.fit_transform(X)\n",
    "  ```\n",
    "\n",
    "* **3. MaxAbs Scaling:**\n",
    "  Scales features by **maximum absolute value**, preserving sparsity.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "  scaler = MaxAbsScaler()\n",
    "  X_scaled = scaler.fit_transform(X)\n",
    "  ```\n",
    "\n",
    "* **4. Robust Scaling:**\n",
    "  Uses **median and IQR**, robust to outliers.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "  scaler = RobustScaler()\n",
    "  X_scaled = scaler.fit_transform(X)\n",
    "  ```\n",
    "\n",
    "ðŸ‘‰ Scaling ensures **all features are on a similar scale**, which improves **model training efficiency and performance**, especially for **distance-based and gradient-based algorithms**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4d3e6f-9f84-4be7-bcd5-f0d616c58aa3",
   "metadata": {},
   "source": [
    "####\n",
    "### 23. What is sklearn.preprocessing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61419db-1097-4d67-80e9-f1dbb776bac5",
   "metadata": {},
   "source": [
    "**`sklearn.preprocessing`** is a module in **Scikit-learn** that provides tools to **prepare and transform data** before feeding it into machine learning models.\n",
    "\n",
    "It includes methods for:\n",
    "\n",
    "* **Scaling & Normalization:** `StandardScaler`, `MinMaxScaler`, `RobustScaler`.\n",
    "* **Encoding Categorical Variables:** `LabelEncoder`, `OneHotEncoder`.\n",
    "* **Handling Missing Values / Imputation:** `SimpleImputer`.\n",
    "* **Generating Polynomial Features:** `PolynomialFeatures`.\n",
    "\n",
    "ðŸ‘‰ These preprocessing steps ensure the data is **in the right format and scale**, improving model performance and training efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d45b99-5780-4d2f-a422-0da6c840a092",
   "metadata": {},
   "source": [
    "####\n",
    "### 24. How do we split data for model fitting (training and testing) in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a8bc26-5470-486f-9a08-1290d0fa6e5e",
   "metadata": {},
   "source": [
    "**`Splitting Data for Model Fitting`** is the process of dividing a dataset into **training and testing sets** to evaluate a machine learning modelâ€™s performance on unseen data.\n",
    "\n",
    "In Python, this is commonly done using **Scikit-learnâ€™s `train_test_split`**:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X = features, y = target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "* **Arguments:**\n",
    "\n",
    "  * `X` â†’ Input features\n",
    "  * `y` â†’ Target variable\n",
    "  * `test_size` â†’ Fraction of data for testing (e.g., 0.2 = 20%)\n",
    "  * `random_state` â†’ Ensures reproducibility\n",
    "\n",
    "* **Purpose:**\n",
    "\n",
    "  * **Training set:** Used to **train/fit** the model\n",
    "  * **Testing set:** Used to **evaluate** model performance on unseen data\n",
    "\n",
    "ðŸ‘‰ Proper splitting helps **prevent overfitting** and gives a **reliable estimate of model performance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fb329-11aa-4044-9177-7a2879fbd4d6",
   "metadata": {},
   "source": [
    "####\n",
    "### 25. Explain data encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b4eb03-b5ef-4055-baf2-2cfd890ce7d5",
   "metadata": {},
   "source": [
    "**`Data Encoding`** is the process of **converting categorical variables into numerical values** so that machine learning models can process them, as most algorithms require numerical input.\n",
    "\n",
    "Common methods:\n",
    "\n",
    "* **Label Encoding:**\n",
    "  Assigns a **unique integer** to each category.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "  le = LabelEncoder()\n",
    "  y_encoded = le.fit_transform(y)  # y = categorical target\n",
    "  ```\n",
    "\n",
    "  *Example:* `['Red', 'Blue', 'Green'] â†’ [0, 1, 2]`\n",
    "\n",
    "* **One-Hot Encoding:**\n",
    "  Creates **binary columns** for each category to avoid implying ordinal relationships.\n",
    "\n",
    "  ```python\n",
    "  from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "  ohe = OneHotEncoder()\n",
    "  X_encoded = ohe.fit_transform(X)  # X = categorical features\n",
    "  ```\n",
    "\n",
    "  *Example:* `['Red', 'Blue'] â†’ [[1,0], [0,1]]`\n",
    "\n",
    "* **Ordinal Encoding:**\n",
    "  Assigns **ordered integers** to categories with a natural ranking.\n",
    "  *Example:* `['Low', 'Medium', 'High'] â†’ [0, 1, 2]`\n",
    "\n",
    "ðŸ‘‰ Data encoding ensures categorical features are **interpretable by ML models** while preserving relationships or avoiding unintended biases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
